{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "912c0071",
   "metadata": {},
   "source": [
    "# Hugging Face TRL GRPO Training for Wordle Game\n",
    "\n",
    "This notebook demonstrates how to use Hugging Face's TRL (Transformers Reinforcement Learning) library with Group Relative Policy Optimization (GRPO) to train a language model for the Wordle game using Parameter-Efficient Fine-Tuning (PEFT).\n",
    "\n",
    "It is inspired by this DeepLearning.AI [course](https://learn.deeplearning.ai/courses/reinforcement-fine-tuning-llms-grpo/).\n",
    "\n",
    "## Overview\n",
    "\n",
    "GRPO (Group Relative Policy Optimization) is a reinforcement learning technique introduced in the DeepSeekMath paper. Unlike traditional RL methods that require value models, GRPO uses a simpler approach by comparing generations within a batch to compute advantages.\n",
    "\n",
    "**Key Components:**\n",
    "- **Base Model**: Qwen/Qwen3-1.7B (small model for efficiency)\n",
    "- **PEFT**: LoRA (Low-Rank Adaptation) for parameter-efficient training\n",
    "- **Reward Functions**: Format checking, feedback usage, and information gain\n",
    "- **Task**: Wordle game with strategic guessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5dc611d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "# Install production dependencies.\n",
    "!pip install --no-cache-dir -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4a6ca80e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "GPU: NVIDIA A100-SXM4-40GB\n",
      "GPU Memory: 39.4 GB\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import datetime\n",
    "from dataclasses import dataclass\n",
    "from enum import Enum\n",
    "from typing import List\n",
    "from sklearn.model_selection import train_test_split\n",
    "# Transformers and TRL imports\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline\n",
    "from datasets import Dataset, load_dataset\n",
    "from trl import GRPOConfig, GRPOTrainer\n",
    "# PEFT imports\n",
    "from peft import get_peft_model, LoraConfig, TaskType\n",
    "\n",
    "# ADD THIS RIGHT HERE - after imports, before any other code\n",
    "import transformers\n",
    "import logging\n",
    "\n",
    "# Set logging levels to reduce verbosity\n",
    "transformers.logging.set_verbosity_error()\n",
    "logging.getLogger(\"transformers\").setLevel(logging.ERROR)\n",
    "\n",
    "# Environment setup\n",
    "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\"\n",
    "# Set device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "# Check GPU memory if available\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f} GB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "818db591",
   "metadata": {},
   "source": [
    "# 1. Data Loading and Preparation\n",
    "We'll use the Predibase Wordle dataset which contains prompts, secret words, and past guess history for training the model to play Wordle strategically.\n",
    "https://huggingface.co/datasets/predibase/wordle-grpo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "525a7922",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Wordle dataset...\n",
      "Total samples in dataset: 76\n",
      "Valid 5-letter alphabetic samples: 75\n",
      "Train set size: 60\n",
      "Validation set size: 15\n"
     ]
    }
   ],
   "source": [
    "def load_and_prepare_data():\n",
    "    \"\"\"Load and prepare the Wordle dataset from Predibase.\"\"\"\n",
    "    print(\"Loading Wordle dataset...\")\n",
    "    dataset = load_dataset(\"predibase/wordle-grpo\", split=\"train\").to_pandas()\n",
    "    \n",
    "    # Filter for valid 5-letter words\n",
    "    valid_rows = dataset[dataset['secret'].astype(str).str.len() == 5]\n",
    "    valid_rows = valid_rows[valid_rows['secret'].str.isalpha()]\n",
    "    \n",
    "    print(f\"Total samples in dataset: {len(dataset)}\")\n",
    "    print(f\"Valid 5-letter alphabetic samples: {len(valid_rows)}\")\n",
    "    \n",
    "    # Split into train/validation\n",
    "    train_rows, val_rows = train_test_split(valid_rows, test_size=0.2, random_state=42)\n",
    "    print(f\"Train set size: {len(train_rows)}\")\n",
    "    print(f\"Validation set size: {len(val_rows)}\")\n",
    "    \n",
    "    # Prepare DataFrames\n",
    "    train_df = train_rows[['prompt', 'secret', 'past_guess_history']].rename(\n",
    "        columns={'secret': 'secret_word'}\n",
    "    ).reset_index(drop=True)\n",
    "    \n",
    "    val_df = val_rows[['prompt', 'secret', 'past_guess_history']].rename(\n",
    "        columns={'secret': 'secret_word'}\n",
    "    ).reset_index(drop=True)\n",
    "    \n",
    "    # Convert to Hugging Face datasets\n",
    "    train_dataset = Dataset.from_pandas(train_df)\n",
    "    val_dataset = Dataset.from_pandas(val_df)\n",
    "    \n",
    "    return train_dataset, val_dataset, train_df, val_df\n",
    "\n",
    "# Load the data\n",
    "train_dataset, val_dataset, train_df, val_df = load_and_prepare_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2537564d",
   "metadata": {},
   "source": [
    "## 2. Model Setup with PEFT\n",
    "\n",
    "We'll load the Qwen3-1.7B model and configure it with LoRA (Low-Rank Adaptation) for efficient fine-tuning. This allows us to train only a small subset of parameters while maintaining good performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "705398d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model: Qwen/Qwen3-1.7B\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "53955170750a4095ab168379d384a842",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded successfully. Model size: 1720.6M parameters\n",
      "Model wrapped with PEFT (LoRA)\n",
      "trainable params: 139,460,608 || all params: 1,860,035,584 || trainable%: 7.4977\n"
     ]
    }
   ],
   "source": [
    "def setup_model_and_tokenizer_peft():\n",
    "    \"\"\"Setup model and tokenizer with LoRA configuration.\"\"\"\n",
    "    # Use a smaller, publicly available model\n",
    "    MODEL_NAME = \"Qwen/Qwen3-1.7B\"\n",
    "    \n",
    "    print(f\"Loading model: {MODEL_NAME}\")\n",
    "    \n",
    "    # Load tokenizer\n",
    "    tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "    \n",
    "    # Load model\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        MODEL_NAME,\n",
    "        torch_dtype=torch.float16,\n",
    "        device_map=\"auto\",\n",
    "        trust_remote_code=True\n",
    "    )\n",
    "    \n",
    "    # Set model to training mode\n",
    "    model.train()\n",
    "    \n",
    "    # Configure tokenizer padding\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "    tokenizer.pad_token_id = tokenizer.eos_token_id\n",
    "    model.config.pad_token_id = tokenizer.pad_token_id\n",
    "    \n",
    "    print(f\"Model loaded successfully. Model size: {sum(p.numel() for p in model.parameters()) / 1e6:.1f}M parameters\")\n",
    "    \n",
    "    # PEFT config (LoRA)\n",
    "    peft_config = LoraConfig(\n",
    "        task_type=TaskType.CAUSAL_LM,\n",
    "        r=128,  # Rank of adaptation\n",
    "        lora_alpha=32,  # LoRA scaling parameter\n",
    "        lora_dropout=0.1,  # LoRA dropout\n",
    "        bias=\"none\",\n",
    "        target_modules=[\n",
    "            \"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\", \n",
    "            \"gate_proj\", \"down_proj\", \"up_proj\"\n",
    "        ],  # Target modules for LoRA\n",
    "    )\n",
    "    \n",
    "    # Apply PEFT to model\n",
    "    model = get_peft_model(model, peft_config)\n",
    "    print(\"Model wrapped with PEFT (LoRA)\")\n",
    "    \n",
    "    # Print trainable parameters info\n",
    "    model.print_trainable_parameters()\n",
    "    \n",
    "    return model, tokenizer\n",
    "\n",
    "# Setup model and tokenizer\n",
    "model, tokenizer = setup_model_and_tokenizer_peft()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4b9b1c0",
   "metadata": {},
   "source": [
    "## 3. Reward Functions\n",
    "\n",
    "We'll define three reward functions based on the provided `reward_functions.py`:\n",
    "1. **Format Check**: Ensures the model outputs follow the correct `<think>` and `<guess>` format\n",
    "2. **Feedback Usage**: Rewards the model for using previous feedback strategically\n",
    "3. **Information Gain**: Rewards guesses that provide maximum information about the secret word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9e08c26b-cee7-47bb-aaab-b9afb9ac59e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We use a Kaggle dataset for format check's reward function\n",
    "# https://www.kaggle.com/datasets/bcruise/wordle-valid-words\n",
    "word_list_path = \"valid_guesses.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d73f75e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reward functions defined successfully!\n"
     ]
    }
   ],
   "source": [
    "# Reward function implementations (aligned to the reference logic)\n",
    "def output_format_check(prompt: str, completion: str, example: dict) -> float:\n",
    "    \"\"\"Check if the output follows the correct <think> and <guess> format.\"\"\"\n",
    "    import re\n",
    "    import pandas as pd\n",
    "\n",
    "    reward = 0.0\n",
    "    try:\n",
    "        # Add synthetic <think> as it's already part of the prompt and prefilled\n",
    "        completion = \"<think>\" + completion\n",
    "\n",
    "        # <think>...</think>\\n<guess>...</guess>\n",
    "        regex = (\n",
    "            r\"^<think>\\s*([^<]*(?:<(?!/?think>)[^<]*)*)\\s*<\\/think>\\n\"\n",
    "            r\"<guess>\\s*([\\s\\S]*?)\\s*<\\/guess>$\"\n",
    "        )\n",
    "        match = re.search(regex, completion, re.DOTALL)\n",
    "        if match is None or len(match.groups()) != 2:\n",
    "            return 0.0\n",
    "\n",
    "        guess = match.groups()[1].strip()\n",
    "\n",
    "        # Length check\n",
    "        if len(guess) != 5:\n",
    "            return 0.1\n",
    "\n",
    "        # Valid word check (use provided path in example)\n",
    "        word_list = pd.read_csv(str(example[\"word_list\"]))\n",
    "        if guess not in word_list[\"Word\"].values:\n",
    "            return 0.5\n",
    "\n",
    "        reward = 1.0\n",
    "    except Exception as e:\n",
    "        # Keep silent/neutral on exceptions as in reference logic\n",
    "        reward = 0.0\n",
    "\n",
    "    return reward\n",
    "\n",
    "\n",
    "def uses_previous_feedback(prompt: str, completion: str, example: dict) -> float:\n",
    "    \"\"\"Check if the guess uses previous feedback strategically.\"\"\"\n",
    "    import re\n",
    "    import ast\n",
    "\n",
    "    reward = 0.0\n",
    "    try:\n",
    "        completion = \"<think>\" + completion\n",
    "\n",
    "        # Extract the guess\n",
    "        regex = r\"<guess>\\s*([\\s\\S]*?)\\s*<\\/guess>$\"\n",
    "        match = re.search(regex, completion, re.DOTALL)\n",
    "        if match is None or len(match.groups()) != 1:\n",
    "            return 0.0\n",
    "\n",
    "        guess = match.groups()[0].strip()\n",
    "        if len(guess) != 5:\n",
    "            return 0.0\n",
    "\n",
    "        past_guess_history = ast.literal_eval(example[\"past_guess_history\"])\n",
    "        if len(past_guess_history) == 0:\n",
    "            return 0.1  # small reward when there's no history to use\n",
    "\n",
    "        correct_letter_to_position = {}\n",
    "        valid_letter_to_position = {}\n",
    "        wrong_letter_to_position = {}\n",
    "\n",
    "        for _, past_feedback in past_guess_history:\n",
    "            tokens = past_feedback.split(\" \")\n",
    "            for i, fb in enumerate(tokens):\n",
    "                # fb examples: \"A(âœ“)\", \"B(-)\", \"C(x)\"\n",
    "                if \"âœ“\" in fb:\n",
    "                    correct_letter_to_position.setdefault(fb[0], set()).add(i)\n",
    "                elif \"-\" in fb:\n",
    "                    valid_letter_to_position.setdefault(fb[0], set()).add(i)\n",
    "                else:\n",
    "                    wrong_letter_to_position.setdefault(fb[0], set()).add(i)\n",
    "\n",
    "        for idx, letter in enumerate(guess):\n",
    "            if letter in correct_letter_to_position and idx in correct_letter_to_position[letter]:\n",
    "                reward += 0.2\n",
    "            elif letter in valid_letter_to_position and idx not in valid_letter_to_position[letter]:\n",
    "                reward += 0.1\n",
    "            elif letter in valid_letter_to_position and idx in valid_letter_to_position[letter]:\n",
    "                reward -= 0.2\n",
    "            elif letter in wrong_letter_to_position:\n",
    "                reward -= 0.5\n",
    "            else:\n",
    "                reward += 0.05  # partial credit for exploring unknowns\n",
    "\n",
    "    except Exception:\n",
    "        return 0.0\n",
    "\n",
    "    return reward\n",
    "\n",
    "\n",
    "def guess_value(prompt: str, completion: str, example: dict) -> float:\n",
    "    \"\"\"Compute normalized information gain of the guess.\"\"\"\n",
    "    import math\n",
    "    import re\n",
    "    import ast\n",
    "    import pandas as pd\n",
    "\n",
    "    def validate_guess(secret: str, guess: str, raw_feedback: bool = False):\n",
    "        feedback = []\n",
    "        secret_list = list(secret)\n",
    "\n",
    "        # Correct positions\n",
    "        for i, (g_char, s_char) in enumerate(zip(guess, secret)):\n",
    "            if g_char == s_char:\n",
    "                feedback.append(f\"{g_char}(âœ“) \")\n",
    "                secret_list[i] = None\n",
    "            else:\n",
    "                feedback.append(None)\n",
    "\n",
    "        # Misplaced / absent\n",
    "        for i, g_char in enumerate(guess):\n",
    "            if feedback[i] is None:\n",
    "                if g_char in secret_list:\n",
    "                    feedback[i] = f\"{g_char}(-) \"\n",
    "                    secret_list[secret_list.index(g_char)] = None\n",
    "                else:\n",
    "                    feedback[i] = f\"{g_char}(x) \"\n",
    "\n",
    "        if raw_feedback:\n",
    "            return feedback\n",
    "        return \"\".join(feedback).strip()\n",
    "\n",
    "    def filter_candidates(all_candidate_words, past_guesses):\n",
    "        filtered = []\n",
    "        for word in all_candidate_words:\n",
    "            valid = True\n",
    "            for past_guess, past_feedback in past_guesses:\n",
    "                if validate_guess(word, past_guess) != past_feedback:\n",
    "                    valid = False\n",
    "                    break\n",
    "            if valid:\n",
    "                filtered.append(word)\n",
    "        return filtered\n",
    "\n",
    "    def compute_normalized_information_gain(all_candidate_words, past_guesses, guess):\n",
    "        candidates = filter_candidates(all_candidate_words, past_guesses)\n",
    "        total = len(candidates)\n",
    "        if total == 0:\n",
    "            return 0.0, 0.0\n",
    "\n",
    "        current_entropy = math.log2(total)\n",
    "\n",
    "        feedback_groups = {}\n",
    "        for word in candidates:\n",
    "            raw = validate_guess(word, guess, raw_feedback=True)\n",
    "            # Pattern: '1' for correct, '0' for misplaced, 'x' for absent\n",
    "            pattern = \"\".join('1' if \"âœ“\" in fb else ('0' if \"-\" in fb else 'x') for fb in raw)\n",
    "            feedback_groups.setdefault(pattern, []).append(word)\n",
    "\n",
    "        expected_entropy = 0.0\n",
    "        max_info_gain = 0.0\n",
    "        for group in feedback_groups.values():\n",
    "            size = len(group)\n",
    "            p = size / total\n",
    "            group_entropy = math.log2(size) if size > 0 else 0.0\n",
    "            expected_entropy += p * group_entropy\n",
    "            info_gain = current_entropy - group_entropy\n",
    "            if info_gain > max_info_gain:\n",
    "                max_info_gain = info_gain\n",
    "\n",
    "        expected_gain = current_entropy - expected_entropy\n",
    "        norm_expected = expected_gain / current_entropy if current_entropy > 0 else 0.0\n",
    "        norm_max = max_info_gain / current_entropy if current_entropy > 0 else 0.0\n",
    "        return norm_expected, norm_max\n",
    "\n",
    "    reward = 0.0\n",
    "    try:\n",
    "        completion = \"<think>\" + completion\n",
    "\n",
    "        # Extract guess\n",
    "        regex = r\"<guess>\\s*([\\s\\S]*?)\\s*<\\/guess>$\"\n",
    "        match = re.search(regex, completion, re.DOTALL)\n",
    "        if match is None or len(match.groups()) != 1:\n",
    "            return 0.0\n",
    "\n",
    "        guess = match.groups()[0].strip()\n",
    "        if len(guess) != 5:\n",
    "            return 0.0\n",
    "\n",
    "        # Word list from example (no case conversion to match reference logic)\n",
    "        word_list = pd.read_csv(str(example[\"word_list\"]))\n",
    "        words = word_list[\"Word\"].values\n",
    "        if guess not in words:\n",
    "            return 0.0\n",
    "\n",
    "        # Past guesses & feedback\n",
    "        past_guess_history = ast.literal_eval(example[\"past_guess_history\"])\n",
    "\n",
    "        # Normalized information gain\n",
    "        normalized_expected_gain, _ = compute_normalized_information_gain(\n",
    "            words, past_guess_history, guess\n",
    "        )\n",
    "        reward = float(normalized_expected_gain)\n",
    "\n",
    "    except Exception:\n",
    "        return 0.0\n",
    "\n",
    "    return reward\n",
    "\n",
    "\n",
    "print(\"Reward functions defined successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "5febc972",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Combined reward function created successfully!\n"
     ]
    }
   ],
   "source": [
    "def wordle_reward_func(completions, prompts=None, secret_word=None, \n",
    "                      past_guess_history=None, model=None, tokenizer=None, **kwargs):\n",
    "    \"\"\"\n",
    "    Combined reward function for Wordle GRPO training.\n",
    "    \n",
    "    Args:\n",
    "        completions: Generated completions from the model\n",
    "        prompts: Input prompts\n",
    "        secret_word: List of secret words for each sample\n",
    "        past_guess_history: Previous guess history for each sample\n",
    "        model: The model (for compatibility)\n",
    "        tokenizer: The tokenizer (for compatibility)\n",
    "    \n",
    "    Returns:\n",
    "        List of rewards for each completion\n",
    "    \"\"\"\n",
    "    rewards = []\n",
    "    \n",
    "    for i in range(len(prompts)):\n",
    "        base_prompt = prompts[i]\n",
    "        secret = secret_word[i]\n",
    "        guess_history = past_guess_history[i] if past_guess_history is not None else []\n",
    "        final_completion = completions[i]\n",
    "        \n",
    "        # Create example dict for reward functions\n",
    "        example = {\n",
    "            'word_list': word_list_path,\n",
    "            'past_guess_history': guess_history,\n",
    "            'secret_word': secret\n",
    "        }\n",
    "        \n",
    "        # Calculate individual rewards\n",
    "        format_reward = output_format_check(base_prompt, final_completion, example)\n",
    "        feedback_reward = uses_previous_feedback(base_prompt, final_completion, example)\n",
    "        info_gain_reward = guess_value(base_prompt, final_completion, example)\n",
    "        \n",
    "        # Combine rewards\n",
    "        total_reward = format_reward + feedback_reward + info_gain_reward\n",
    "        \n",
    "        print(f\"Sample {i}: format={format_reward:.2f}, feedback={feedback_reward:.2f}, \"\n",
    "              f\"info_gain={info_gain_reward:.2f}, total={total_reward:.2f}\")\n",
    "        \n",
    "        rewards.append(total_reward)\n",
    "    \n",
    "    print(f\"Batch rewards: {rewards}\")\n",
    "    return rewards\n",
    "\n",
    "\n",
    "# Create a wrapper function with model and tokenizer\n",
    "def reward_func_with_model(*args, **kwargs):\n",
    "    return wordle_reward_func(*args, model=model, tokenizer=tokenizer, **kwargs)\n",
    "\n",
    "# Set function name for TRL\n",
    "reward_func_with_model.__name__ = \"wordle_reward_func\"\n",
    "\n",
    "print(\"Combined reward function created successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca3bdcfb",
   "metadata": {},
   "source": [
    "## 4. GRPO Training Configuration\n",
    "\n",
    "Now we'll configure the GRPO training parameters. GRPO (Group Relative Policy Optimization) compares generations within a batch to compute advantages, eliminating the need for a separate value model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "576fa517",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = GRPOConfig(\n",
    "    output_dir=\"outputs/wordle-grpo-peft\",\n",
    "    num_train_epochs=50,\n",
    "    per_device_train_batch_size=4,\n",
    "    per_device_eval_batch_size=8,\n",
    "    gradient_accumulation_steps=4,\n",
    "    num_generations=8,\n",
    "    learning_rate=1e-5,\n",
    "    logging_steps=50,\n",
    "    eval_strategy=\"steps\",\n",
    "    eval_steps=50,\n",
    "    bf16=False,\n",
    "    fp16=True,\n",
    "    remove_unused_columns=False,\n",
    "    max_prompt_length=1024,\n",
    "    max_completion_length=2048,\n",
    "    seed=42,\n",
    "    gradient_checkpointing=False,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"eval_loss\",\n",
    "    greater_is_better=False,\n",
    "    logging_dir=\"outputs/wordle-grpo-peft/logs\",\n",
    "    report_to=[\"tensorboard\", \"wandb\"],\n",
    "    run_name=f\"wordle-grpo-peft-{datetime.datetime.now().strftime('%Y%m%d-%H%M%S')}\",\n",
    "    temperature=0.7,\n",
    "    top_p=0.95,\n",
    "    top_k=40,\n",
    "    repetition_penalty=1.1,\n",
    "    generation_kwargs={\n",
    "        \"temperature\": 0.7,\n",
    "        \"top_p\": 0.95,\n",
    "        \"top_k\": 40,\n",
    "        \"repetition_penalty\": 1.1\n",
    "    },\n",
    "    scale_rewards=False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f20b86c",
   "metadata": {},
   "source": [
    "# 5. Training Execution\n",
    "Now let's create the GRPO trainer and start the training process. The trainer will use our reward functions to optimize the model's Wordle-playing strategy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "8ab5193a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output directory created: outputs/wordle-grpo-peft\n"
     ]
    }
   ],
   "source": [
    "trainer = GRPOTrainer(\n",
    "    model=model,\n",
    "    reward_funcs=reward_func_with_model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset,\n",
    "    processing_class=tokenizer,\n",
    ")\n",
    "\n",
    "# Create output directory if it doesn't exist\n",
    "os.makedirs(training_args.output_dir, exist_ok=True)\n",
    "print(f\"Output directory created: {training_args.output_dir}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b720557",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start training\n",
    "print(\"ðŸš€ Starting GRPO training...\")\n",
    "print(\"This may take a while depending on your hardware and dataset size.\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "try:\n",
    "    # Train the model\n",
    "    trainer.train()\n",
    "    print(\"âœ… Training completed successfully!\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"âŒ Training failed with error: {e}\")\n",
    "    print(\"This might be due to memory constraints or other issues.\")\n",
    "    print(\"Try reducing batch_size or num_generations if you encounter OOM errors.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f7cd54d",
   "metadata": {},
   "source": [
    "## 6. Model Saving\n",
    "\n",
    "After training, we'll save the model and tokenizer for later use. We'll save both the final model and the best checkpoint if available."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12c36646",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the final model\n",
    "final_model_dir = os.path.join(training_args.output_dir, \"final_model\")\n",
    "os.makedirs(final_model_dir, exist_ok=True)\n",
    "\n",
    "print(\"ðŸ’¾ Saving final model...\")\n",
    "model.save_pretrained(final_model_dir)\n",
    "tokenizer.save_pretrained(final_model_dir)\n",
    "print(f\"Final model saved to: {final_model_dir}\")\n",
    "\n",
    "# Save the best model if available\n",
    "if hasattr(trainer, 'state') and hasattr(trainer.state, 'best_model_checkpoint'):\n",
    "    best_ckpt = trainer.state.best_model_checkpoint\n",
    "    if best_ckpt:\n",
    "        print(f\"ðŸ“Š Best model checkpoint found at: {best_ckpt}\")\n",
    "        best_model_dir = os.path.join(training_args.output_dir, \"best_model\")\n",
    "        os.makedirs(best_model_dir, exist_ok=True)\n",
    "        \n",
    "        # Load and save best model\n",
    "        try:\n",
    "            best_model = AutoModelForCausalLM.from_pretrained(best_ckpt)\n",
    "            best_model.save_pretrained(best_model_dir)\n",
    "            tokenizer.save_pretrained(best_model_dir)\n",
    "            print(f\"Best model saved to: {best_model_dir}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Could not load best checkpoint: {e}\")\n",
    "    else:\n",
    "        print(\"âš ï¸ No best model checkpoint found.\")\n",
    "else:\n",
    "    print(\"âš ï¸ Trainer does not have best_model_checkpoint attribute.\")\n",
    "\n",
    "print(\"\\nâœ… Model saving completed!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0898954",
   "metadata": {},
   "source": [
    "## 7. Model Evaluation and Testing\n",
    "\n",
    "Let's test our trained model on some Wordle examples to see how well it has learned to play the game."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b78d7eba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utility functions for testing\n",
    "def extract_guess_from_completion(completion: str) -> str:\n",
    "    \"\"\"Extract the guess from model completion.\"\"\"\n",
    "    import re\n",
    "    match = re.search(r\"\\s*([\\s\\S]*?)\\s*<\\/guess>\", completion, re.DOTALL)\n",
    "    if not match:\n",
    "        return \"\"\n",
    "    return match.group(1).strip().upper()\n",
    "\n",
    "def test_model_on_sample(model, tokenizer, prompt, max_new_tokens=256):\n",
    "    \"\"\"Test the model on a single prompt.\"\"\"\n",
    "    # Tokenize input\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\", padding=True, truncation=True)\n",
    "    inputs = {k: v.to(model.device) for k, v in inputs.items()}\n",
    "    \n",
    "    # Generate completion\n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=max_new_tokens,\n",
    "            temperature=0.7,\n",
    "            top_p=0.9,\n",
    "            do_sample=True,\n",
    "            pad_token_id=tokenizer.pad_token_id,\n",
    "        )\n",
    "    \n",
    "    # Decode and extract completion\n",
    "    generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    completion = generated_text[len(prompt):].strip()\n",
    "    \n",
    "    return completion\n",
    "\n",
    "# Test on a few samples\n",
    "print(\"ðŸŽ¯ Testing the trained model...\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Get a few test samples\n",
    "test_samples = val_dataset.select(range(min(3, len(val_dataset))))\n",
    "\n",
    "for i, sample in enumerate(test_samples):\n",
    "    print(f\"\\n--- Test Sample {i+1} ---\")\n",
    "    print(f\"Secret word: {sample['secret_word']}\")\n",
    "    print(f\"Past guesses: {sample['past_guess_history']}\")\n",
    "    \n",
    "    # Get model completion\n",
    "    prompt = sample['prompt']\n",
    "    completion = test_model_on_sample(model, tokenizer, prompt)\n",
    "    \n",
    "    print(f\"Model completion: {completion}\")\n",
    "    \n",
    "    # Extract guess\n",
    "    guess = extract_guess_from_completion(completion)\n",
    "    print(f\"Extracted guess: {guess}\")\n",
    "    \n",
    "    # Calculate reward\n",
    "    example = {\n",
    "        'word_list': word_list_path,\n",
    "        'past_guess_history': sample['past_guess_history'],\n",
    "        'secret_word': sample['secret_word']\n",
    "    }\n",
    "    \n",
    "    format_reward = output_format_check(prompt, completion, example)\n",
    "    feedback_reward = uses_previous_feedback(prompt, completion, example)\n",
    "    info_gain_reward = guess_value(prompt, completion, example)\n",
    "    total_reward = format_reward + feedback_reward + info_gain_reward\n",
    "    \n",
    "    print(f\"Rewards - Format: {format_reward:.2f}, Feedback: {feedback_reward:.2f}, \"\n",
    "          f\"Info Gain: {info_gain_reward:.2f}, Total: {total_reward:.2f}\")\n",
    "    print(\"-\" * 40)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a8496b0",
   "metadata": {},
   "source": [
    "## 8. Conclusion and Next Steps\n",
    "\n",
    "### What We Accomplished\n",
    "\n",
    "In this notebook, we successfully:\n",
    "\n",
    "1. **Loaded and prepared** the Wordle dataset from Predibase\n",
    "2. **Set up a small language model** (Qwen3-1.7B) with LoRA for efficient fine-tuning\n",
    "3. **Implemented reward functions** that evaluate:\n",
    "   - Format compliance (correct use of `<think>` and `<guess>` tags)\n",
    "   - Strategic use of previous feedback\n",
    "   - Information value of guesses\n",
    "4. **Configured and ran GRPO training** using TRL (Transformers Reinforcement Learning)\n",
    "5. **Tested the trained model** on Wordle examples\n",
    "6. **Created an interactive game** to see the model in action\n",
    "\n",
    "### Key Benefits of GRPO\n",
    "\n",
    "- **No value model needed**: Unlike PPO, GRPO doesn't require a separate value model\n",
    "- **Group-based optimization**: Uses batch comparisons for more stable training\n",
    "- **Scalable**: Can handle complex reasoning tasks efficiently\n",
    "- **Compatible with PEFT**: Works well with LoRA for resource-efficient training\n",
    "\n",
    "### Next Steps for Improvement\n",
    "\n",
    "1. **Larger model**: Try with Qwen3-4B or larger models for better reasoning\n",
    "2. **More training data**: Use the full dataset instead of a subset\n",
    "3. **Advanced reward functions**: Implement the full information gain calculation\n",
    "4. **Hyperparameter tuning**: Experiment with learning rates, generation parameters\n",
    "5. **Multi-turn evaluation**: Test on complete Wordle games with multiple rounds\n",
    "6. **Deployment**: Save and deploy the model for real-world use\n",
    "\n",
    "### Resources\n",
    "\n",
    "- [TRL Documentation](https://huggingface.co/docs/trl)\n",
    "- [GRPO Paper (DeepSeekMath)](https://arxiv.org/abs/2402.03300)\n",
    "- [PEFT Documentation](https://huggingface.co/docs/peft)\n",
    "- [Hugging Face Cookbook](https://huggingface.co/learn/cookbook)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
