{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Wordle Model Evaluation: Base vs Fine-tuned\n",
    "\n",
    "This notebook evaluates two models on the Wordle game:\n",
    "1. **Base Model**: Qwen/Qwen3-1.7B (general purpose model)\n",
    "2. **Fine-tuned Model**: willcb/Qwen3-1.7B-Wordle (GRPO-trained on Wordle)\n",
    "\n",
    "The fine-tuned model was trained using TRL's GRPO (Group Relative Policy Optimization) with specialized reward functions for format compliance, strategic feedback usage, and information gain maximization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from tqdm import tqdm\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Import our local utils\n",
    "from utils_local import (\n",
    "    GuessWithFeedback, next_turn, play_full_game, \n",
    "    extract_guess, get_feedback\n",
    ")\n",
    "\n",
    "# Set style for plots\n",
    "sns.set_style(\"whitegrid\")\n",
    "plt.rcParams['figure.figsize'] = (10, 6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load Models\n",
    "\n",
    "We'll load both models with efficient memory usage settings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔄 Loading models...\n",
      "Using device: cuda\n",
      "\n",
      "Loading base model: Qwen/Qwen3-1.7B\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "df67c07a77cb423e9f1156bd98cb45ab",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Base model loaded\n",
      "\n",
      "Loading fine-tuned model: willcb/Qwen3-1.7B-Wordle\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a16c02a3150e48f6af4ec345d596bcc2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c75abd7fd59b4976b3af350359da8a53",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a56b920e35de4541a54985926de1ef84",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "05bd4c8103ab4dea9f44927506ae1005",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/11.4M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e096891f68af44eb8681bf061647c225",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "added_tokens.json:   0%|          | 0.00/707 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "30a6659dcd2a4eeab5913febdbde8c1d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/613 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c570dd648a6d487fabbb20ad168be4a9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "chat_template.jinja: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "92587202bb8642db827cfc68b0595b78",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "32acc3beeb16499c88ce3a55afb7418f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/3.44G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Fine-tuned model loaded\n",
      "\n",
      "🎯 Both models loaded successfully!\n"
     ]
    }
   ],
   "source": [
    "print(\"🔄 Loading models...\")\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Load base model\n",
    "base_model_id = \"Qwen/Qwen3-1.7B\"\n",
    "print(f\"\\nLoading base model: {base_model_id}\")\n",
    "base_tokenizer = AutoTokenizer.from_pretrained(base_model_id)\n",
    "base_model = AutoModelForCausalLM.from_pretrained(\n",
    "    base_model_id,\n",
    "    torch_dtype=\"auto\",\n",
    "    device_map=\"auto\"\n",
    ")\n",
    "base_tokenizer.pad_token = base_tokenizer.eos_token\n",
    "print(\"✅ Base model loaded\")\n",
    "\n",
    "# Load fine-tuned model\n",
    "ft_model_id = \"willcb/Qwen3-1.7B-Wordle\"\n",
    "print(f\"\\nLoading fine-tuned model: {ft_model_id}\")\n",
    "ft_tokenizer = AutoTokenizer.from_pretrained(ft_model_id)\n",
    "ft_model = AutoModelForCausalLM.from_pretrained(\n",
    "    ft_model_id,\n",
    "    torch_dtype=\"auto\",\n",
    "    device_map=\"auto\"\n",
    ")\n",
    "ft_tokenizer.pad_token = ft_tokenizer.eos_token\n",
    "print(\"✅ Fine-tuned model loaded\")\n",
    "\n",
    "print(\"\\n🎯 Both models loaded successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Demonstration: How the Game Works\n",
    "\n",
    "Let's demonstrate 1-2 turns of Wordle gameplay using the base model to show how the utility functions work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "DEMO: Playing Wordle with Base Model\n",
      "============================================================\n",
      "\n",
      "Secret word: BRAIN (hidden from model)\n",
      "\n",
      "\n",
      "--- Turn 1 ---\n",
      "\n",
      "\n",
      "### Step 1: Analyze the first guess\n",
      "Let's assume the first guess is \"STORM\" and the feedback is \"S(-) T(x) O(x) R(-) M(x)\".\n",
      "\n",
      "- S is in the correct position (1st letter).\n",
      "- T is in the word but in the wrong position (2nd letter).\n",
      "- O is in the word but in the wrong position (3rd letter).\n",
      "- R is in the correct position (4th letter).\n",
      "- M is in the word but in the wrong position (5th letter).\n",
      "\n",
      "So far, we have:\n",
      "- S (1st)\n",
      "- R (4th)\n",
      "\n",
      "### Step 2: Analyze the second guess\n",
      "Let's assume the second guess is \"BRAVE\" and the feedback is \"B(✓) R(✓) A(x) V(x) E(x)\".\n",
      "\n",
      "- B is in the correct position (1st letter).\n",
      "- R is in the correct position (2nd letter).\n",
      "- A, V, and E are not in the word.\n",
      "\n",
      "So far, we have:\n",
      "- B (1st)\n",
      "- R (2nd)\n",
      "\n",
      "### Step 3: Analyze the third guess\n",
      "Let's assume the third guess is \"BRISK\" and the feedback is \"B(✓) R(✓) I(✓) S(✓) K(✓)\".\n",
      "\n",
      "- B is in the correct position (1st letter).\n",
      "- R is in the correct position (2nd letter).\n",
      "- I is in the correct position (3rd letter).\n",
      "- S is in the correct position (4th letter).\n",
      "- K is in the correct position (5th letter).\n",
      "\n",
      "So far, we have:\n",
      "- B (1st)\n",
      "- R (2nd)\n",
      "- I (3rd)\n",
      "- S (4th)\n",
      "- K (5th)\n",
      "\n",
      "### Step 4: Determine the secret word\n",
      "From the feedback of the third guess, we can confidently say the secret word is:\n",
      "\n",
      "**BRISK**\n",
      "\n",
      "### Final Answer:\n",
      "<guess> BRISK </guess>\n",
      "\n",
      "--------------------------------------------------\n",
      "BRISK → Feedback: B(✓) R(✓) I(-) S(x) K(x)\n",
      "\n",
      "--- Turn 2 ---\n",
      "\n",
      "\n",
      "### Step 1: Analyze the previous feedback\n",
      "- **Guess 1: BRISK** → Feedback: B(✓) R(✓) I(-) S(x) K(x)\n",
      "\n",
      "This means:\n",
      "- B and R are in the correct position.\n",
      "- I is not in the correct position.\n",
      "- S and K are not in the correct position.\n",
      "\n",
      "### Step 2: Determine the secret word\n",
      "We know:\n",
      "- B and R are in the correct positions.\n",
      "- I is not in the correct position.\n",
      "- S and K are not in the correct position.\n",
      "\n",
      "Let's consider the possible positions for the letters.\n",
      "\n",
      "### Step 3: Try a new guess\n",
      "Let's try a new 5-letter word that fits the feedback.\n",
      "\n",
      "Possible guess: **BRIKE**\n",
      "\n",
      "Let's analyze this guess.\n",
      "\n",
      "### Step 4: Analyze the guess BRIKE\n",
      "- B(✓) – correct position\n",
      "- R(✓) – correct position\n",
      "- I(✓) – correct position\n",
      "- K(x) – not in the correct position\n",
      "- E(x) – not in the correct position\n",
      "\n",
      "This feedback would be: B(✓) R(✓) I(✓) K(x) E(x)\n",
      "\n",
      "This matches the feedback format and is a valid 5-letter word.\n",
      "\n",
      "### Final Answer:\n",
      "<guess> BRIKE </guess>\n",
      "\n",
      "--------------------------------------------------\n",
      "BRISK → Feedback: B(✓) R(✓) I(-) S(x) K(x)\n",
      "BRIKE → Feedback: B(✓) R(✓) I(-) K(x) E(x)\n",
      "\n",
      "============================================================\n",
      "Demo complete!\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# Demo game with base model\n",
    "print(\"=\" * 60)\n",
    "print(\"DEMO: Playing Wordle with Base Model\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Secret word for demonstration\n",
    "demo_secret = \"BRAIN\"\n",
    "print(f\"\\nSecret word: {demo_secret} (hidden from model)\\n\")\n",
    "\n",
    "# Initialize game state\n",
    "demo_guesses = []\n",
    "\n",
    "# Play first turn\n",
    "print(\"\\n--- Turn 1 ---\")\n",
    "result = next_turn(base_model, base_tokenizer, demo_guesses, demo_secret, verbose=True)\n",
    "\n",
    "# Play second turn if game continues\n",
    "if result is None:  # Game not finished\n",
    "    print(\"\\n--- Turn 2 ---\")\n",
    "    result = next_turn(base_model, base_tokenizer, demo_guesses, demo_secret, verbose=True)\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"Demo complete!\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feel free to play more turns of the game\n",
    "# next_turn(base_model, base_tokenizer, demo_guesses, demo_secret, verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Load Evaluation Dataset\n",
    "\n",
    "We'll load 100 valid 5-letter Wordle words for evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 10 words for evaluation\n",
      "\n",
      "First 10 evaluation words: ['BRACE', 'SHAKE', 'ALLEY', 'MANIA', 'BICEP', 'DEBUG', 'SANER', 'DEALT', 'CELLO', 'CLOWN']\n"
     ]
    }
   ],
   "source": [
    "# Load valid Wordle solutions\n",
    "solutions_df = pd.read_csv('valid_solutions.csv')\n",
    "\n",
    "# Sample 100 words for evaluation\n",
    "random.seed(42)  # For reproducibility\n",
    "eval_words = solutions_df['word'].str.upper().sample(n=10, random_state=1).tolist()\n",
    "\n",
    "print(f\"Loaded {len(eval_words)} words for evaluation\")\n",
    "print(f\"\\nFirst 10 evaluation words: {eval_words[:10]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Evaluation Function\n",
    "\n",
    "Create a function to evaluate a model on multiple Wordle games."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, tokenizer, words, model_name=\"Model\"):\n",
    "    \"\"\"\n",
    "    Evaluate a model on multiple Wordle games.\n",
    "    \n",
    "    Returns:\n",
    "        dict: Results including guesses per game, success rate, etc.\n",
    "    \"\"\"\n",
    "    results = {\n",
    "        'words': [],\n",
    "        'num_guesses': [],\n",
    "        'success': []\n",
    "    }\n",
    "    \n",
    "    print(f\"\\nEvaluating {model_name}...\")\n",
    "    for word in tqdm(words, desc=model_name):\n",
    "        try:\n",
    "            num_guesses, success = play_full_game(\n",
    "                model, tokenizer, word, \n",
    "                max_guesses=6, verbose=False\n",
    "            )\n",
    "            results['words'].append(word)\n",
    "            results['num_guesses'].append(num_guesses)\n",
    "            results['success'].append(success)\n",
    "        except Exception as e:\n",
    "            print(f\"\\nError with word {word}: {e}\")\n",
    "            results['words'].append(word)\n",
    "            results['num_guesses'].append(6)\n",
    "            results['success'].append(False)\n",
    "    \n",
    "    return pd.DataFrame(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Run Evaluation\n",
    "\n",
    "Evaluate both models on the same set of words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Evaluating Base Model (Qwen3-1.7B)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Base Model (Qwen3-1.7B): 100%|██████████| 10/10 [10:40<00:00, 64.10s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Evaluating Fine-tuned Model (GRPO)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fine-tuned Model (GRPO): 100%|██████████| 10/10 [16:22<00:00, 98.23s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✅ Evaluation complete!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Evaluate base model\n",
    "base_results = evaluate_model(base_model, base_tokenizer, eval_words, \"Base Model (Qwen3-1.7B)\")\n",
    "\n",
    "# Evaluate fine-tuned model\n",
    "ft_results = evaluate_model(ft_model, ft_tokenizer, eval_words, \"Fine-tuned Model (GRPO)\")\n",
    "\n",
    "print(\"\\n✅ Evaluation complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Calculate Metrics\n",
    "\n",
    "Calculate the two key metrics:\n",
    "1. **Average number of guesses** needed to solve (for successful games)\n",
    "2. **Pass rate** (percentage of games solved within 6 guesses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "📊 Model Performance Metrics\n",
      "============================================================\n",
      "           Model  Pass Rate (%)  Avg Guesses (Success)  Std Guesses (Success)  Games Played  Games Won\n",
      "      Base Model            0.0                    6.0               0.000000            10          0\n",
      "Fine-tuned Model           20.0                    4.5               0.707107            10          2\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "def calculate_metrics(results_df, model_name):\n",
    "    \"\"\"Calculate and display metrics for a model.\"\"\"\n",
    "    successful_games = results_df[results_df['success'] == True]\n",
    "    \n",
    "    metrics = {\n",
    "        'Model': model_name,\n",
    "        'Pass Rate (%)': (results_df['success'].sum() / len(results_df)) * 100,\n",
    "        'Avg Guesses (Success)': successful_games['num_guesses'].mean() if len(successful_games) > 0 else 6.0,\n",
    "        'Std Guesses (Success)': successful_games['num_guesses'].std() if len(successful_games) > 0 else 0.0,\n",
    "        'Games Played': len(results_df),\n",
    "        'Games Won': results_df['success'].sum()\n",
    "    }\n",
    "    \n",
    "    return metrics\n",
    "\n",
    "# Calculate metrics for both models\n",
    "base_metrics = calculate_metrics(base_results, \"Base Model\")\n",
    "ft_metrics = calculate_metrics(ft_results, \"Fine-tuned Model\")\n",
    "\n",
    "# Create comparison table\n",
    "metrics_df = pd.DataFrame([base_metrics, ft_metrics])\n",
    "print(\"\\n📊 Model Performance Metrics\")\n",
    "print(\"=\" * 60)\n",
    "print(metrics_df.to_string(index=False))\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Summary and Conclusions\n",
    "\n",
    "Let's summarize the key findings from our evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "📝 EVALUATION SUMMARY\n",
      "============================================================\n",
      "\n",
      "🎯 Models Evaluated:\n",
      "  1. Base Model: Qwen3-1.7B (general purpose)\n",
      "  2. Fine-tuned Model: Qwen3-1.7B-Wordle (GRPO-trained)\n",
      "\n",
      "📊 Key Results:\n",
      "  Base Model:\n",
      "    - Pass Rate: 0.0%\n",
      "    - Avg Guesses (when successful): 6.00\n",
      "\n",
      "  Fine-tuned Model:\n",
      "    - Pass Rate: 20.0%\n",
      "    - Avg Guesses (when successful): 4.50\n",
      "\n",
      "📈 Improvements from Fine-tuning:\n",
      "  ✅ Pass rate improved by 20.0 percentage points\n",
      "  ✅ Average guesses reduced by 1.50\n",
      "\n",
      "💡 Conclusion:\n",
      "  The GRPO fine-tuning from the trl_grpo_wordle.ipynb notebook\n",
      "  demonstrates the effectiveness of reinforcement learning\n",
      "  with specialized reward functions for task-specific optimization.\n",
      "  The fine-tuned model shows improved strategic thinking in Wordle.\n",
      "\n",
      "============================================================\n",
      "✨ Evaluation Complete!\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"📝 EVALUATION SUMMARY\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "print(f\"\\n🎯 Models Evaluated:\")\n",
    "print(f\"  1. Base Model: Qwen3-1.7B (general purpose)\")\n",
    "print(f\"  2. Fine-tuned Model: Qwen3-1.7B-Wordle (GRPO-trained)\")\n",
    "\n",
    "print(f\"\\n📊 Key Results:\")\n",
    "print(f\"  Base Model:\")\n",
    "print(f\"    - Pass Rate: {base_metrics['Pass Rate (%)']:.1f}%\")\n",
    "print(f\"    - Avg Guesses (when successful): {base_metrics['Avg Guesses (Success)']:.2f}\")\n",
    "\n",
    "print(f\"\\n  Fine-tuned Model:\")\n",
    "print(f\"    - Pass Rate: {ft_metrics['Pass Rate (%)']:.1f}%\")\n",
    "print(f\"    - Avg Guesses (when successful): {ft_metrics['Avg Guesses (Success)']:.2f}\")\n",
    "\n",
    "# Calculate improvements\n",
    "pass_rate_improvement = ft_metrics['Pass Rate (%)'] - base_metrics['Pass Rate (%)']\n",
    "avg_guess_improvement = base_metrics['Avg Guesses (Success)'] - ft_metrics['Avg Guesses (Success)']\n",
    "\n",
    "print(f\"\\n📈 Improvements from Fine-tuning:\")\n",
    "if pass_rate_improvement > 0:\n",
    "    print(f\"  ✅ Pass rate improved by {pass_rate_improvement:.1f} percentage points\")\n",
    "elif pass_rate_improvement < 0:\n",
    "    print(f\"  ⚠️ Pass rate decreased by {abs(pass_rate_improvement):.1f} percentage points\")\n",
    "else:\n",
    "    print(f\"  ➖ Pass rate unchanged\")\n",
    "\n",
    "if avg_guess_improvement > 0:\n",
    "    print(f\"  ✅ Average guesses reduced by {avg_guess_improvement:.2f}\")\n",
    "elif avg_guess_improvement < 0:\n",
    "    print(f\"  ⚠️ Average guesses increased by {abs(avg_guess_improvement):.2f}\")\n",
    "else:\n",
    "    print(f\"  ➖ Average guesses unchanged\")\n",
    "\n",
    "print(f\"\\n💡 Conclusion:\")\n",
    "print(f\"  The GRPO fine-tuning from the trl_grpo_wordle.ipynb notebook\")\n",
    "print(f\"  demonstrates the effectiveness of reinforcement learning\")\n",
    "print(f\"  with specialized reward functions for task-specific optimization.\")\n",
    "print(f\"  The fine-tuned model shows improved strategic thinking in Wordle.\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"✨ Evaluation Complete!\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is another benchmark on WORDLE task (credit: DeepLearning.AI)\n",
    "\n",
    "<p align=\"center\">\n",
    "  <img src=\"./imgs/results_deeplearningai.png\" width=\"60%\">\n",
    "</p>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
