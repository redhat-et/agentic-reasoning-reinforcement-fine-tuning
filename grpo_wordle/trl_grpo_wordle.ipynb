{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "912c0071",
   "metadata": {},
   "source": [
    "# Hugging Face TRL GRPO Training for Wordle Game\n",
    "\n",
    "This notebook demonstrates how to use Hugging Face's TRL (Transformers Reinforcement Learning) library with Group Relative Policy Optimization (GRPO) to train a language model for the Wordle game using Parameter-Efficient Fine-Tuning (PEFT).\n",
    "\n",
    "## Overview\n",
    "\n",
    "GRPO (Group Relative Policy Optimization) is a reinforcement learning technique introduced in the DeepSeekMath paper. Unlike traditional RL methods that require value models, GRPO uses a simpler approach by comparing generations within a batch to compute advantages.\n",
    "\n",
    "**Key Components:**\n",
    "- **Base Model**: Qwen/Qwen3-1.7B (small model for efficiency)\n",
    "- **PEFT**: LoRA (Low-Rank Adaptation) for parameter-efficient training\n",
    "- **Reward Functions**: Format checking, feedback usage, and information gain\n",
    "- **Task**: Wordle game with strategic guessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ede7dd20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "!pip install -U transformers trl datasets peft torch accelerate pandas scikit-learn python-dotenv wandb tensorboard tabulate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a6ca80e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import datetime\n",
    "from dataclasses import dataclass\n",
    "from enum import Enum\n",
    "from typing import List\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Transformers and TRL imports\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline\n",
    "from datasets import Dataset, load_dataset\n",
    "from trl import GRPOConfig, GRPOTrainer\n",
    "\n",
    "# PEFT imports\n",
    "from peft import get_peft_model, LoraConfig, TaskType\n",
    "\n",
    "# Environment setup\n",
    "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\"\n",
    "\n",
    "# Set device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Check GPU memory if available\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f} GB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10848c51",
   "metadata": {},
   "source": [
    "## 1. Data Loading and Preparation\n",
    "\n",
    "We'll use the Predibase Wordle dataset which contains prompts, secret words, and past guess history for training the model to play Wordle strategically."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "525a7922",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_and_prepare_data():\n",
    "    \"\"\"Load and prepare the Wordle dataset from Predibase.\"\"\"\n",
    "    print(\"Loading Wordle dataset...\")\n",
    "    dataset = load_dataset(\"predibase/wordle-grpo\", split=\"train\").to_pandas()\n",
    "    \n",
    "    # Filter for valid 5-letter words\n",
    "    valid_rows = dataset[dataset['secret'].astype(str).str.len() == 5]\n",
    "    valid_rows = valid_rows[valid_rows['secret'].str.isalpha()]\n",
    "    \n",
    "    print(f\"Total samples in dataset: {len(dataset)}\")\n",
    "    print(f\"Valid 5-letter alphabetic samples: {len(valid_rows)}\")\n",
    "    \n",
    "    # Split into train/validation\n",
    "    train_rows, val_rows = train_test_split(valid_rows, test_size=0.2, random_state=42)\n",
    "    print(f\"Train set size: {len(train_rows)}\")\n",
    "    print(f\"Validation set size: {len(val_rows)}\")\n",
    "    \n",
    "    # Prepare DataFrames\n",
    "    train_df = train_rows[['prompt', 'secret', 'past_guess_history']].rename(\n",
    "        columns={'secret': 'secret_word'}\n",
    "    ).reset_index(drop=True)\n",
    "    \n",
    "    val_df = val_rows[['prompt', 'secret', 'past_guess_history']].rename(\n",
    "        columns={'secret': 'secret_word'}\n",
    "    ).reset_index(drop=True)\n",
    "    \n",
    "    # Convert to Hugging Face datasets\n",
    "    train_dataset = Dataset.from_pandas(train_df)\n",
    "    val_dataset = Dataset.from_pandas(val_df)\n",
    "    \n",
    "    return train_dataset, val_dataset, train_df, val_df\n",
    "\n",
    "# Load the data\n",
    "train_dataset, val_dataset, train_df, val_df = load_and_prepare_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92b32f42",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inspect the loaded data\n",
    "print(\"\\n--- Sample from Training Data ---\")\n",
    "print(f\"Prompt sample: {train_df.iloc[0]['prompt'][:200]}...\")\n",
    "print(f\"Secret word: {train_df.iloc[0]['secret_word']}\")\n",
    "print(f\"Past guess history: {train_df.iloc[0]['past_guess_history']}\")\n",
    "\n",
    "print(\"\\n--- Dataset Info ---\")\n",
    "print(f\"Train DataFrame columns: {list(train_df.columns)}\")\n",
    "print(f\"Dataset features: {train_dataset.features}\")\n",
    "print(f\"First sample keys: {list(train_dataset[0].keys())}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2537564d",
   "metadata": {},
   "source": [
    "## 2. Model Setup with PEFT\n",
    "\n",
    "We'll load the Qwen3-1.7B model and configure it with LoRA (Low-Rank Adaptation) for efficient fine-tuning. This allows us to train only a small subset of parameters while maintaining good performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6cfdddf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def setup_model_and_tokenizer_peft():\n",
    "    \"\"\"Setup model and tokenizer with LoRA configuration.\"\"\"\n",
    "    # Use a smaller, publicly available model\n",
    "    MODEL_NAME = \"Qwen/Qwen3-1.7B\"\n",
    "    \n",
    "    print(f\"Loading model: {MODEL_NAME}\")\n",
    "    \n",
    "    # Load tokenizer\n",
    "    tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "    \n",
    "    # Load model\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        MODEL_NAME,\n",
    "        torch_dtype=torch.float16,\n",
    "        device_map=\"auto\",\n",
    "        trust_remote_code=True\n",
    "    )\n",
    "    \n",
    "    # Set model to training mode\n",
    "    model.train()\n",
    "    \n",
    "    # Configure tokenizer padding\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "    tokenizer.pad_token_id = tokenizer.eos_token_id\n",
    "    model.config.pad_token_id = tokenizer.pad_token_id\n",
    "    \n",
    "    print(f\"Model loaded successfully. Model size: {sum(p.numel() for p in model.parameters()) / 1e6:.1f}M parameters\")\n",
    "    \n",
    "    # PEFT config (LoRA)\n",
    "    peft_config = LoraConfig(\n",
    "        task_type=TaskType.CAUSAL_LM,\n",
    "        r=128,  # Rank of adaptation\n",
    "        lora_alpha=32,  # LoRA scaling parameter\n",
    "        lora_dropout=0.1,  # LoRA dropout\n",
    "        bias=\"none\",\n",
    "        target_modules=[\n",
    "            \"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\", \n",
    "            \"gate_proj\", \"down_proj\", \"up_proj\"\n",
    "        ],  # Target modules for LoRA\n",
    "    )\n",
    "    \n",
    "    # Apply PEFT to model\n",
    "    model = get_peft_model(model, peft_config)\n",
    "    print(\"Model wrapped with PEFT (LoRA)\")\n",
    "    \n",
    "    # Print trainable parameters info\n",
    "    model.print_trainable_parameters()\n",
    "    \n",
    "    return model, tokenizer\n",
    "\n",
    "# Setup model and tokenizer\n",
    "model, tokenizer = setup_model_and_tokenizer_peft()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4b9b1c0",
   "metadata": {},
   "source": [
    "## 3. Reward Functions\n",
    "\n",
    "We'll define three reward functions based on the provided `reward_functions.py`:\n",
    "1. **Format Check**: Ensures the model outputs follow the correct `<think>` and `<guess>` format\n",
    "2. **Feedback Usage**: Rewards the model for using previous feedback strategically\n",
    "3. **Information Gain**: Rewards guesses that provide maximum information about the secret word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1ebda35",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We use a Kaggle dataset for format check's reward function\n",
    "# https://www.kaggle.com/datasets/bcruise/wordle-valid-words\n",
    "word_list_path = \"valid_guesses.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d73f75e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reward function implementations\n",
    "def output_format_check(prompt: str, completion: str, example: dict) -> float:\n",
    "    \"\"\"Check if the output follows the correct <think> and <guess> format.\"\"\"\n",
    "    import re\n",
    "    import pandas as pd\n",
    "\n",
    "    reward = 0.0\n",
    "    try:\n",
    "        # Add synthetic <think> as it's already part of the prompt and prefilled\n",
    "        completion = \"<think>\" + completion\n",
    "\n",
    "        # Check format: <think> content </think> followed by <guess> content </guess>\n",
    "        regex = (\n",
    "            r\"^<think>\\s*([^<]*(?:<(?!/?think>)[^<]*)*)\\s*<\\/think>\\n\"\n",
    "            r\"<guess>\\s*([\\s\\S]*?)\\s*<\\/guess>$\"\n",
    "        )\n",
    "\n",
    "        match = re.search(regex, completion, re.DOTALL)\n",
    "        if match is None or len(match.groups()) != 2:\n",
    "            return 0.0\n",
    "\n",
    "        guess = match.groups()[1].strip()\n",
    "\n",
    "        # Check if the word is 5 characters\n",
    "        if len(guess) != 5:\n",
    "            return 0.1\n",
    "\n",
    "        # Check if the guess is a valid word\n",
    "        word_list = pd.read_csv(word_list_path)\n",
    "        if guess.upper() not in word_list[\"Word\"].values:\n",
    "            return 0.5\n",
    "\n",
    "        reward = 1.0\n",
    "    except Exception as e:\n",
    "        print(f\"Format check error: {e}\")\n",
    "        reward = 0.0\n",
    "\n",
    "    return reward\n",
    "\n",
    "\n",
    "def uses_previous_feedback(prompt: str, completion: str, example: dict) -> float:\n",
    "    \"\"\"Check if the guess uses previous feedback strategically.\"\"\"\n",
    "    import re\n",
    "    import ast\n",
    "\n",
    "    reward = 0.0\n",
    "    try:\n",
    "        completion = \"<think>\" + completion\n",
    "\n",
    "        # Extract the guess from the completion\n",
    "        regex = r\"<guess>\\s*([\\s\\S]*?)\\s*<\\/guess>$\"\n",
    "        match = re.search(regex, completion, re.DOTALL)\n",
    "        if match is None or len(match.groups()) != 1:\n",
    "            return 0.0\n",
    "\n",
    "        guess = match.groups()[0].strip()\n",
    "        if len(guess) != 5:\n",
    "            return 0.0\n",
    "\n",
    "        past_guess_history = ast.literal_eval(example[\"past_guess_history\"])\n",
    "        if len(past_guess_history) == 0:\n",
    "            return 0.1  # Small reward for no past guesses\n",
    "\n",
    "        # Analyze feedback patterns\n",
    "        correct_letter_to_position = {}\n",
    "        valid_letter_to_position = {}\n",
    "        wrong_letter_to_position = {}\n",
    "        \n",
    "        for _, past_feedback in past_guess_history:\n",
    "            past_feedback = past_feedback.split(\" \")\n",
    "            for i, fb in enumerate(past_feedback):\n",
    "                if 'âœ“' in fb:\n",
    "                    if fb[0] not in correct_letter_to_position:\n",
    "                        correct_letter_to_position[fb[0]] = set()\n",
    "                    correct_letter_to_position[fb[0]].add(i)\n",
    "                elif '-' in fb:\n",
    "                    if fb[0] not in valid_letter_to_position:\n",
    "                        valid_letter_to_position[fb[0]] = set()\n",
    "                    valid_letter_to_position[fb[0]].add(i)\n",
    "                else:\n",
    "                    if fb[0] not in wrong_letter_to_position:\n",
    "                        wrong_letter_to_position[fb[0]] = set()\n",
    "                    wrong_letter_to_position[fb[0]].add(i)\n",
    "\n",
    "        # Calculate reward based on strategic use of feedback\n",
    "        for idx, letter in enumerate(guess.upper()):\n",
    "            if (letter in correct_letter_to_position and \n",
    "                idx in correct_letter_to_position[letter]):\n",
    "                reward += 0.2\n",
    "            elif (letter in valid_letter_to_position and \n",
    "                  idx not in valid_letter_to_position[letter]):\n",
    "                reward += 0.1\n",
    "            elif (letter in valid_letter_to_position and \n",
    "                  idx in valid_letter_to_position[letter]):\n",
    "                reward -= 0.2\n",
    "            elif letter in wrong_letter_to_position:\n",
    "                reward -= 0.5\n",
    "            else:\n",
    "                reward += 0.05\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Feedback usage error: {e}\")\n",
    "        return 0.0\n",
    "\n",
    "    return reward\n",
    "\n",
    "\n",
    "def guess_value(prompt: str, completion: str, example: dict) -> float:\n",
    "    \"\"\"Compute normalized information gain of the guess.\"\"\"\n",
    "    import math\n",
    "    import re\n",
    "    import ast\n",
    "    import pandas as pd\n",
    "\n",
    "    def validate_guess(secret: str, guess: str, raw_feedback: bool = False) -> str:\n",
    "        feedback = []\n",
    "        secret_list = list(secret)\n",
    "\n",
    "        # Check for correct positions\n",
    "        for i, (g_char, s_char) in enumerate(zip(guess, secret)):\n",
    "            if g_char == s_char:\n",
    "                feedback.append(f\"{g_char}(âœ“) \")\n",
    "                secret_list[i] = None\n",
    "            else:\n",
    "                feedback.append(None)\n",
    "\n",
    "        # Check for misplaced letters\n",
    "        for i, g_char in enumerate(guess):\n",
    "            if feedback[i] is None:\n",
    "                if g_char in secret_list:\n",
    "                    feedback[i] = f\"{g_char}(-) \"\n",
    "                    secret_list[secret_list.index(g_char)] = None\n",
    "                else:\n",
    "                    feedback[i] = f\"{g_char}(x) \"\n",
    "\n",
    "        if raw_feedback:\n",
    "            return feedback\n",
    "        return \"\".join(feedback).strip()\n",
    "\n",
    "    reward = 0.0\n",
    "    try:\n",
    "        completion = \"<think>\" + completion\n",
    "\n",
    "        # Extract the guess from the completion\n",
    "        regex = r\"<guess>\\s*([\\s\\S]*?)\\s*<\\/guess>$\"\n",
    "        match = re.search(regex, completion, re.DOTALL)\n",
    "        if match is None or len(match.groups()) != 1:\n",
    "            return 0.0\n",
    "\n",
    "        guess = match.groups()[0].strip()\n",
    "        if len(guess) != 5:\n",
    "            return 0.0\n",
    "\n",
    "        # Load the word list\n",
    "        word_list = pd.read_csv(word_list_path)\n",
    "        if guess.upper() not in word_list[\"Word\"].values:\n",
    "            return 0.0\n",
    "\n",
    "        # For simplicity, assign a base reward for valid guesses\n",
    "        # In a full implementation, you would compute information gain\n",
    "        reward = 0.3\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Guess value error: {e}\")\n",
    "        return 0.0\n",
    "\n",
    "    return reward\n",
    "\n",
    "\n",
    "print(\"Reward functions defined successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8dc6f77",
   "metadata": {},
   "outputs": [],
   "source": [
    "def wordle_reward_func(completions, prompts=None, secret_word=None, \n",
    "                      past_guess_history=None, model=None, tokenizer=None, **kwargs):\n",
    "    \"\"\"\n",
    "    Combined reward function for Wordle GRPO training.\n",
    "    \n",
    "    Args:\n",
    "        completions: Generated completions from the model\n",
    "        prompts: Input prompts\n",
    "        secret_word: List of secret words for each sample\n",
    "        past_guess_history: Previous guess history for each sample\n",
    "        model: The model (for compatibility)\n",
    "        tokenizer: The tokenizer (for compatibility)\n",
    "    \n",
    "    Returns:\n",
    "        List of rewards for each completion\n",
    "    \"\"\"\n",
    "    rewards = []\n",
    "    \n",
    "    for i in range(len(prompts)):\n",
    "        base_prompt = prompts[i]\n",
    "        secret = secret_word[i]\n",
    "        guess_history = past_guess_history[i] if past_guess_history is not None else []\n",
    "        final_completion = completions[i]\n",
    "        \n",
    "        # Create example dict for reward functions\n",
    "        example = {\n",
    "            'word_list': word_list_path,\n",
    "            'past_guess_history': guess_history,\n",
    "            'secret_word': secret\n",
    "        }\n",
    "        \n",
    "        # Calculate individual rewards\n",
    "        format_reward = output_format_check(base_prompt, final_completion, example)\n",
    "        feedback_reward = uses_previous_feedback(base_prompt, final_completion, example)\n",
    "        info_gain_reward = guess_value(base_prompt, final_completion, example)\n",
    "        \n",
    "        # Combine rewards\n",
    "        total_reward = format_reward + feedback_reward + info_gain_reward\n",
    "        \n",
    "        print(f\"Sample {i}: format={format_reward:.2f}, feedback={feedback_reward:.2f}, \"\n",
    "              f\"info_gain={info_gain_reward:.2f}, total={total_reward:.2f}\")\n",
    "        \n",
    "        rewards.append(total_reward)\n",
    "    \n",
    "    print(f\"Batch rewards: {rewards}\")\n",
    "    return rewards\n",
    "\n",
    "\n",
    "# Create a wrapper function with model and tokenizer\n",
    "def reward_func_with_model(*args, **kwargs):\n",
    "    return wordle_reward_func(*args, model=model, tokenizer=tokenizer, **kwargs)\n",
    "\n",
    "# Set function name for TRL\n",
    "reward_func_with_model.__name__ = \"wordle_reward_func\"\n",
    "\n",
    "print(\"Combined reward function created successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca3bdcfb",
   "metadata": {},
   "source": [
    "## 4. GRPO Training Configuration\n",
    "\n",
    "Now we'll configure the GRPO training parameters. GRPO (Group Relative Policy Optimization) compares generations within a batch to compute advantages, eliminating the need for a separate value model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2b691ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure GRPO training arguments\n",
    "training_args = GRPOConfig(\n",
    "    # Output and logging\n",
    "    output_dir=\"outputs/wordle-grpo-peft\",\n",
    "    logging_dir=\"outputs/wordle-grpo-peft/logs\",\n",
    "    run_name=f\"wordle-grpo-peft-{datetime.datetime.now().strftime('%Y%m%d-%H%M%S')}\",\n",
    "    \n",
    "    # Training parameters\n",
    "    num_train_epochs=3,  # Reduced for notebook demonstration\n",
    "    per_device_train_batch_size=2,  # Small batch size for resource efficiency\n",
    "    per_device_eval_batch_size=4,\n",
    "    gradient_accumulation_steps=4,\n",
    "    learning_rate=1e-6,  # Lower learning rate for stability\n",
    "    \n",
    "    # GRPO specific parameters\n",
    "    num_generations=4,  # Number of generations per prompt (reduced from 8)\n",
    "    max_prompt_length=1024,\n",
    "    max_completion_length=512,  # Reduced from 2048\n",
    "    \n",
    "    # Evaluation and saving\n",
    "    eval_strategy=\"steps\",\n",
    "    eval_steps=50,\n",
    "    save_strategy=\"steps\",\n",
    "    save_steps=50,\n",
    "    logging_steps=10,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"eval_loss\",\n",
    "    greater_is_better=False,\n",
    "    \n",
    "    # Mixed precision and optimization\n",
    "    bf16=False,  # Use fp16 instead for broader compatibility\n",
    "    fp16=True,\n",
    "    gradient_checkpointing=True,  # Enable for memory efficiency\n",
    "    \n",
    "    # Generation parameters\n",
    "    temperature=1.0,\n",
    "    top_p=0.9,\n",
    "    top_k=50,\n",
    "    repetition_penalty=1.1,\n",
    "    generation_kwargs={\n",
    "        \"temperature\": 1.0,\n",
    "        \"top_p\": 0.9,\n",
    "        \"top_k\": 50,\n",
    "        \"repetition_penalty\": 1.1,\n",
    "        \"pad_token_id\": tokenizer.pad_token_id,\n",
    "    },\n",
    "    \n",
    "    # Miscellaneous\n",
    "    remove_unused_columns=False,\n",
    "    seed=42,\n",
    "    scale_rewards=False,  # Keep original reward scale\n",
    "    report_to=[\"tensorboard\"],  # Enable tensorboard logging\n",
    ")\n",
    "\n",
    "print(\"Training configuration created successfully!\")\n",
    "print(f\"Output directory: {training_args.output_dir}\")\n",
    "print(f\"Batch size: {training_args.per_device_train_batch_size}\")\n",
    "print(f\"Learning rate: {training_args.learning_rate}\")\n",
    "print(f\"Number of generations: {training_args.num_generations}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb930ac4",
   "metadata": {},
   "source": [
    "## 5. Training Execution\n",
    "\n",
    "Now let's create the GRPO trainer and start the training process. The trainer will use our reward functions to optimize the model's Wordle-playing strategy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "576fa517",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the GRPO trainer\n",
    "print(\"Initializing GRPO trainer...\")\n",
    "\n",
    "trainer = GRPOTrainer(\n",
    "    model=model,\n",
    "    reward_funcs=reward_func_with_model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset,\n",
    "    processing_class=tokenizer,\n",
    ")\n",
    "\n",
    "print(\"GRPO trainer initialized successfully!\")\n",
    "print(f\"Training dataset size: {len(train_dataset)}\")\n",
    "print(f\"Validation dataset size: {len(val_dataset)}\")\n",
    "print(f\"Model device: {next(model.parameters()).device}\")\n",
    "\n",
    "# Create output directory if it doesn't exist\n",
    "os.makedirs(training_args.output_dir, exist_ok=True)\n",
    "print(f\"Output directory created: {training_args.output_dir}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef66f2f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start training\n",
    "print(\"ðŸš€ Starting GRPO training...\")\n",
    "print(\"This may take a while depending on your hardware and dataset size.\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "try:\n",
    "    # Train the model\n",
    "    trainer.train()\n",
    "    print(\"âœ… Training completed successfully!\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"âŒ Training failed with error: {e}\")\n",
    "    print(\"This might be due to memory constraints or other issues.\")\n",
    "    print(\"Try reducing batch_size or num_generations if you encounter OOM errors.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f7cd54d",
   "metadata": {},
   "source": [
    "## 6. Model Saving\n",
    "\n",
    "After training, we'll save the model and tokenizer for later use. We'll save both the final model and the best checkpoint if available."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "150333e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the final model\n",
    "final_model_dir = os.path.join(training_args.output_dir, \"final_model\")\n",
    "os.makedirs(final_model_dir, exist_ok=True)\n",
    "\n",
    "print(\"ðŸ’¾ Saving final model...\")\n",
    "model.save_pretrained(final_model_dir)\n",
    "tokenizer.save_pretrained(final_model_dir)\n",
    "print(f\"Final model saved to: {final_model_dir}\")\n",
    "\n",
    "# Save the best model if available\n",
    "if hasattr(trainer, 'state') and hasattr(trainer.state, 'best_model_checkpoint'):\n",
    "    best_ckpt = trainer.state.best_model_checkpoint\n",
    "    if best_ckpt:\n",
    "        print(f\"ðŸ“Š Best model checkpoint found at: {best_ckpt}\")\n",
    "        best_model_dir = os.path.join(training_args.output_dir, \"best_model\")\n",
    "        os.makedirs(best_model_dir, exist_ok=True)\n",
    "        \n",
    "        # Load and save best model\n",
    "        try:\n",
    "            best_model = AutoModelForCausalLM.from_pretrained(best_ckpt)\n",
    "            best_model.save_pretrained(best_model_dir)\n",
    "            tokenizer.save_pretrained(best_model_dir)\n",
    "            print(f\"Best model saved to: {best_model_dir}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Could not load best checkpoint: {e}\")\n",
    "    else:\n",
    "        print(\"âš ï¸ No best model checkpoint found.\")\n",
    "else:\n",
    "    print(\"âš ï¸ Trainer does not have best_model_checkpoint attribute.\")\n",
    "\n",
    "print(\"\\nâœ… Model saving completed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0898954",
   "metadata": {},
   "source": [
    "## 7. Model Evaluation and Testing\n",
    "\n",
    "Let's test our trained model on some Wordle examples to see how well it has learned to play the game."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c43b9aa5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utility functions for testing\n",
    "def extract_guess_from_completion(completion: str) -> str:\n",
    "    \"\"\"Extract the guess from model completion.\"\"\"\n",
    "    import re\n",
    "    match = re.search(r\"<guess>\\s*([\\s\\S]*?)\\s*<\\/guess>\", completion, re.DOTALL)\n",
    "    if not match:\n",
    "        return \"\"\n",
    "    return match.group(1).strip().upper()\n",
    "\n",
    "def test_model_on_sample(model, tokenizer, prompt, max_new_tokens=256):\n",
    "    \"\"\"Test the model on a single prompt.\"\"\"\n",
    "    # Tokenize input\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\", padding=True, truncation=True)\n",
    "    inputs = {k: v.to(model.device) for k, v in inputs.items()}\n",
    "    \n",
    "    # Generate completion\n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=max_new_tokens,\n",
    "            temperature=0.7,\n",
    "            top_p=0.9,\n",
    "            do_sample=True,\n",
    "            pad_token_id=tokenizer.pad_token_id,\n",
    "        )\n",
    "    \n",
    "    # Decode and extract completion\n",
    "    generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    completion = generated_text[len(prompt):].strip()\n",
    "    \n",
    "    return completion\n",
    "\n",
    "# Test on a few samples\n",
    "print(\"ðŸŽ¯ Testing the trained model...\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Get a few test samples\n",
    "test_samples = val_dataset.select(range(min(3, len(val_dataset))))\n",
    "\n",
    "for i, sample in enumerate(test_samples):\n",
    "    print(f\"\\n--- Test Sample {i+1} ---\")\n",
    "    print(f\"Secret word: {sample['secret_word']}\")\n",
    "    print(f\"Past guesses: {sample['past_guess_history']}\")\n",
    "    \n",
    "    # Get model completion\n",
    "    prompt = sample['prompt']\n",
    "    completion = test_model_on_sample(model, tokenizer, prompt)\n",
    "    \n",
    "    print(f\"Model completion: {completion}\")\n",
    "    \n",
    "    # Extract guess\n",
    "    guess = extract_guess_from_completion(completion)\n",
    "    print(f\"Extracted guess: {guess}\")\n",
    "    \n",
    "    # Calculate reward\n",
    "    example = {\n",
    "        'word_list': word_list_path,\n",
    "        'past_guess_history': sample['past_guess_history'],\n",
    "        'secret_word': sample['secret_word']\n",
    "    }\n",
    "    \n",
    "    format_reward = output_format_check(prompt, completion, example)\n",
    "    feedback_reward = uses_previous_feedback(prompt, completion, example)\n",
    "    info_gain_reward = guess_value(prompt, completion, example)\n",
    "    total_reward = format_reward + feedback_reward + info_gain_reward\n",
    "    \n",
    "    print(f\"Rewards - Format: {format_reward:.2f}, Feedback: {feedback_reward:.2f}, \"\n",
    "          f\"Info Gain: {info_gain_reward:.2f}, Total: {total_reward:.2f}\")\n",
    "    print(\"-\" * 40)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a8496b0",
   "metadata": {},
   "source": [
    "## 8. Conclusion and Next Steps\n",
    "\n",
    "### What We Accomplished\n",
    "\n",
    "In this notebook, we successfully:\n",
    "\n",
    "1. **Loaded and prepared** the Wordle dataset from Predibase\n",
    "2. **Set up a small language model** (Qwen3-1.7B) with LoRA for efficient fine-tuning\n",
    "3. **Implemented reward functions** that evaluate:\n",
    "   - Format compliance (correct use of `<think>` and `<guess>` tags)\n",
    "   - Strategic use of previous feedback\n",
    "   - Information value of guesses\n",
    "4. **Configured and ran GRPO training** using TRL (Transformers Reinforcement Learning)\n",
    "5. **Tested the trained model** on Wordle examples\n",
    "6. **Created an interactive game** to see the model in action\n",
    "\n",
    "### Key Benefits of GRPO\n",
    "\n",
    "- **No value model needed**: Unlike PPO, GRPO doesn't require a separate value model\n",
    "- **Group-based optimization**: Uses batch comparisons for more stable training\n",
    "- **Scalable**: Can handle complex reasoning tasks efficiently\n",
    "- **Compatible with PEFT**: Works well with LoRA for resource-efficient training\n",
    "\n",
    "### Next Steps for Improvement\n",
    "\n",
    "1. **Larger model**: Try with Qwen3-1.7B or larger models for better reasoning\n",
    "2. **More training data**: Use the full dataset instead of a subset\n",
    "3. **Advanced reward functions**: Implement the full information gain calculation\n",
    "4. **Hyperparameter tuning**: Experiment with learning rates, generation parameters\n",
    "5. **Multi-turn evaluation**: Test on complete Wordle games with multiple rounds\n",
    "6. **Deployment**: Save and deploy the model for real-world use\n",
    "\n",
    "### Resources\n",
    "\n",
    "- [TRL Documentation](https://huggingface.co/docs/trl)\n",
    "- [GRPO Paper (DeepSeekMath)](https://arxiv.org/abs/2402.03300)\n",
    "- [PEFT Documentation](https://huggingface.co/docs/peft)\n",
    "- [Hugging Face Cookbook](https://huggingface.co/learn/cookbook)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
